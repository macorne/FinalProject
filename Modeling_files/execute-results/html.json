{
  "hash": "fd660791e86b1b21d304e346f5b7afeb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modeling\"\nauthor: \"Matthew Corne\"\ndate: \"12-05-2024\"\nformat: html\neditor: visual\n---\n\n\n## Introduction\n\nWrite an intro!  We will include (at least) 5 predictors in this model:  (ORIGINALS! age, sex, income, smoker, and heart disease or attack) sex, income, stroke, heart disease or attack, and heavy alcohol consumption.\n\nWe will load the libraries and read in the data here:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(baguette)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: parsnip\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(corrplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ncorrplot 0.95 loaded\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(lubridate)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'lubridate'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ranger)\n#library(randomForest)\nlibrary(stringr)\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tibble       3.2.1\n✔ dplyr        1.1.4     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ purrr        1.0.2     ✔ workflowsets 1.1.0\n✔ recipes      1.1.0     ✔ yardstick    1.3.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()  masks scales::discard()\n✖ tidyr::expand()   masks Matrix::expand()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ tidyr::pack()     masks Matrix::pack()\n✖ recipes::step()   masks stats::step()\n✖ tidyr::unpack()   masks Matrix::unpack()\n✖ recipes::update() masks Matrix::update(), stats::update()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ readr   2.1.5\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ tidyr::expand()     masks Matrix::expand()\n✖ dplyr::filter()     masks stats::filter()\n✖ recipes::fixed()    masks stringr::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ tidyr::pack()       masks Matrix::pack()\n✖ readr::spec()       masks yardstick::spec()\n✖ tidyr::unpack()     masks Matrix::unpack()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(vroom)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'vroom'\n\nThe following objects are masked from 'package:readr':\n\n    as.col_spec, col_character, col_date, col_datetime, col_double,\n    col_factor, col_guess, col_integer, col_logical, col_number,\n    col_skip, col_time, cols, cols_condense, cols_only, date_names,\n    date_names_lang, date_names_langs, default_locale, fwf_cols,\n    fwf_empty, fwf_positions, fwf_widths, locale, output_column,\n    problems, spec\n\nThe following object is masked from 'package:yardstick':\n\n    spec\n\nThe following object is masked from 'package:scales':\n\n    col_factor\n```\n\n\n:::\n\n```{.r .cell-code}\n#diabetes_binary_health_indicators_BRFSS2015.csv\n#dbhi = diabetes binary health indicators\ndata <- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             <dbl>  <dbl>    <dbl>     <dbl> <dbl>  <dbl>  <dbl>\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack <dbl>, PhysActivity <dbl>,\n#   Fruits <dbl>, Veggies <dbl>, HvyAlcoholConsump <dbl>, AnyHealthcare <dbl>,\n#   NoDocbcCost <dbl>, GenHlth <dbl>, MentHlth <dbl>, PhysHlth <dbl>,\n#   DiffWalk <dbl>, Sex <dbl>, Age <dbl>, Education <dbl>, Income <dbl>\n```\n\n\n:::\n:::\n\n\nNext, we will convert variables to factor variables, where appropriate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data |>\n  mutate(Diabetes_binary=\n           factor(Diabetes_binary,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no diabetes\",\"prediabetes or diabetes\")),\n         HighBP=\n           factor(HighBP,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no high BP\",\"high BP\")),\n         HighChol=\n           factor(HighChol,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no high cholesterol\",\"high cholesterol\")),\n         CholCheck=\n           factor(CholCheck,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no cholesterol check in 5 years\",\"yes cholesterol check in 5 years\")),\n         #BMI=factor(BMI),\n         Smoker=\n           factor(Smoker,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no\",\"yes\")),\n         Stroke=\n           factor(Stroke,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no\",\"yes\")),\n         HeartDiseaseorAttack=\n           factor(HeartDiseaseorAttack,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no\",\"yes\")),\n         PhysActivity=\n           factor(PhysActivity,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no\",\"yes\")),\n         Fruits=\n           factor(Fruits,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no\",\"yes\")),\n         Veggies=\n           factor(Veggies,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no\",\"yes\")),\n         HvyAlcoholConsump=\n           factor(HvyAlcoholConsump,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no\",\"yes\")),\n         AnyHealthcare=\n           factor(AnyHealthcare,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no\",\"yes\")),\n         NoDocbcCost=\n           factor(NoDocbcCost,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no\",\"yes\")),\n         GenHlth=\n           factor(GenHlth,\n                  levels=c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                  labels=c(\"excellent\",\"very good\",\"good\",\"fair\",\"poor\")),\n         #MentHlth=\n           #factor(MentHlth),\n         #PhysHlth=\n           #factor(PhysHlth),\n         DiffWalk=\n           factor(DiffWalk,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"no difficulty walking\",\"yes difficulty walking\")),\n         Sex=\n           factor(Sex,\n                  levels=c(\"0\",\"1\"),\n                  labels=c(\"female\",\"male\")),\n         Age=\n           factor(Age,\n                  levels=c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\"),\n                  labels=c(\"18 to 24\",\"25 to 29\",\"30 to 34\",\"35 to 39\",\"40 to 44\",\"45 to 49\",\"50 to 54\",\"55 to 59\",\"60 to 64\",\"65 to 69\",\"70 to 74\",\"75 to 79\",\"80 or older\")),\n         Education=\n           factor(Education,\n                  levels=c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"),\n                  labels=c(\"Never attended school or only kindergarten\",\"Grades 1 through 8 (Elementary)\",\"Grades 9 through 11 (Some high school)\",\"Grade 12 or GED (High school graduate)\",\"College 1 year to 3 years (Some college or technical school)\",\"College 4 years or more (College graduate)\")),\n         Income=\n           factor(Income,\n                  levels=c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"),\n                  labels=c(\"Less than $10,000\",\"$10,000 to less than $15,000\",\"$15,000 to less than $20,000\",\"$20,000 to less than $25,000\",\"$25,000 to less than $35,000\",\"$35,000 to less than $50,000\",\"$50,000 to less than $75,000\",\"$75,000 or more\"))\n         )\n```\n:::\n\n\nNow, subset the data to look at the 5 predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbhi_data <- data #|> \n#  select(Diabetes_binary,Age,HeartDiseaseorAttack,Income,Sex,Smoker)\ndbhi_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 253,680 × 22\n   Diabetes_binary         HighBP     HighChol     CholCheck   BMI Smoker Stroke\n   <fct>                   <fct>      <fct>        <fct>     <dbl> <fct>  <fct> \n 1 no diabetes             high BP    high choles… yes chol…    40 yes    no    \n 2 no diabetes             no high BP no high cho… no chole…    25 yes    no    \n 3 no diabetes             high BP    high choles… yes chol…    28 no     no    \n 4 no diabetes             high BP    no high cho… yes chol…    27 no     no    \n 5 no diabetes             high BP    high choles… yes chol…    24 no     no    \n 6 no diabetes             high BP    high choles… yes chol…    25 yes    no    \n 7 no diabetes             high BP    no high cho… yes chol…    30 yes    no    \n 8 no diabetes             high BP    high choles… yes chol…    25 yes    no    \n 9 prediabetes or diabetes high BP    high choles… yes chol…    30 yes    no    \n10 no diabetes             no high BP no high cho… yes chol…    24 no     no    \n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack <fct>, PhysActivity <fct>,\n#   Fruits <fct>, Veggies <fct>, HvyAlcoholConsump <fct>, AnyHealthcare <fct>,\n#   NoDocbcCost <fct>, GenHlth <fct>, MentHlth <dbl>, PhysHlth <dbl>,\n#   DiffWalk <fct>, Sex <fct>, Age <fct>, Education <fct>, Income <fct>\n```\n\n\n:::\n:::\n\n\n\n## Split the Data\n\nSet the seed.  Then, use functions from tidymodels to split the data into a training and test set (70/30 split). Then, use the strata argument to stratify the split on the `Sex` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(11)\ndbhi_split <- initial_split(dbhi_data, prop = 0.70, strata=Sex) #strata = argument goes in the parentheses, if needed\ndbhi_train <- training(dbhi_split)\ndbhi_test <- testing(dbhi_split)\n```\n:::\n\n\nWe will perform 5-fold cross validation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbhi_5_fold <- vfold_cv(dbhi_train, 5)\n\ndbhi_5_fold\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits                 id   \n  <list>                 <chr>\n1 <split [142060/35515]> Fold1\n2 <split [142060/35515]> Fold2\n3 <split [142060/35515]> Fold3\n4 <split [142060/35515]> Fold4\n5 <split [142060/35515]> Fold5\n```\n\n\n:::\n:::\n\n\n## Models\n\nWe will consider two kinds of models:  classification tree and random forest.  We will ...\n\nGet a recipe.   Then, standardize the numeric variables since their scales are pretty different. Finally, create dummy variables for the predictors since they need to be numeric (again).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#bystanders <- colnames(dbhi_data)[c(2:4,7:18,21)]\n#bystanders\n\n#dbhi_train\n#Diabetes_binary ~ Age + BMI + Income + Sex + Smoker\n#ORIGINALS:  update_role(HighBP,HighChol,CholCheck,Stroke,HeartDiseaseorAttack,PhysActivity,Fruits,Veggies,HvyAlcoholConsump,AnyHealthcare,NoDocbcCost,GenHlth,MentHlth,PhysHlth,DiffWalk,Education,new_role = \"bystander\") \ndbhi_recipe <- recipe(Diabetes_binary ~ ., data = dbhi_train) |>\n  update_role(Age,Smoker,BMI,HighBP,HighChol,CholCheck,PhysActivity,Fruits,Veggies,AnyHealthcare,NoDocbcCost,GenHlth,MentHlth,PhysHlth,DiffWalk,Education,new_role = \"bystander\") |>\n  step_dummy(all_nominal_predictors()) |>\n  step_normalize(all_numeric(), -all_outcomes()) #|>\n#  summary()\n#  prep(training = dbhi_train) |>\n#  bake(dbhi_train)\n\ndbhi_recipe\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor:  5\nbystander: 16\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: all_nominal_predictors()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Centering and scaling for: all_numeric() and -all_outcomes()\n```\n\n\n:::\n:::\n\n\n### Classification Tree\n\nYou should provide a thorough explanation of what a classification tree model is. Then you should fit a classification tree with varying values for the complexity parameter and choose the best model (based on 5 fold CV on the training set). Include at least 5 predictors in this model.\n\nFirst, tell `tidymodels` that we are performing a classification task:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_mod <- decision_tree(tree_depth = tune(),\n                          min_n = 10,\n                          cost_complexity = tune()) |>\n  set_engine(\"rpart\") |>\n  set_mode(\"classification\")\n```\n:::\n\n\nNext, create a workflow to use in the fitting process:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_wkf <- workflow() |>\n  add_recipe(dbhi_recipe) |>\n  add_model(tree_mod)\n```\n:::\n\n\nThen, to create the tuning grid for fitting our models, we use cross validation (CV) to select the tuning parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp <- tree_wkf |> \n  tune_grid(resamples = dbhi_5_fold, metrics=metric_set(mn_log_loss))\ntemp |> \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             <dbl>      <int> <chr>       <chr>      <dbl> <int>   <dbl> <chr>  \n 1        1.41e- 5          6 mn_log_loss binary     0.397     5 3.21e-3 Prepro…\n 2        9.46e- 3          9 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 3        1.05e-10         11 mn_log_loss binary     0.389     5 4.51e-4 Prepro…\n 4        2.89e- 4          3 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 5        2.97e- 8         13 mn_log_loss binary     0.389     5 4.51e-4 Prepro…\n 6        7.51e- 8          8 mn_log_loss binary     0.392     5 2.56e-3 Prepro…\n 7        5.06e- 5         14 mn_log_loss binary     0.399     5 2.28e-3 Prepro…\n 8        2.87e- 9          4 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 9        5.92e- 2          1 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n10        1.02e- 6         11 mn_log_loss binary     0.389     5 4.51e-4 Prepro…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_grid <- grid_regular(cost_complexity(),\n                          #min_n(),\n                          tree_depth(),\n                          levels = c(5, 5))\n\ntree_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             <dbl>      <int>\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n```\n\n\n:::\n:::\n\n\nThis generates 25 (5x5) candidate decision tree models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_fits <- tree_wkf |> \n  tune_grid(resamples = dbhi_5_fold,\n            metrics=metric_set(mn_log_loss),\n            grid = tree_grid)\ntree_fits |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             <dbl>      <int> <chr>       <chr>      <dbl> <int>   <dbl> <chr>  \n 1    0.0000000001          1 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 2    0.0000000178          1 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 3    0.00000316            1 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 4    0.000562              1 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 5    0.1                   1 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 6    0.0000000001          4 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 7    0.0000000178          4 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 8    0.00000316            4 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n 9    0.000562              4 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n10    0.1                   4 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n# ℹ 15 more rows\n```\n\n\n:::\n:::\n\n\n\nCombine the metrics across the folds, then plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_fits |>\n  collect_metrics() |>\n  mutate(tree_depth = factor(tree_depth)) |>\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Modeling_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntree_fits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits                 id    .metrics          .notes          \n  <list>                 <chr> <list>            <list>          \n1 <split [142060/35515]> Fold1 <tibble [25 × 6]> <tibble [0 × 3]>\n2 <split [142060/35515]> Fold2 <tibble [25 × 6]> <tibble [0 × 3]>\n3 <split [142060/35515]> Fold3 <tibble [25 × 6]> <tibble [0 × 3]>\n4 <split [142060/35515]> Fold4 <tibble [25 × 6]> <tibble [0 × 3]>\n5 <split [142060/35515]> Fold5 <tibble [25 × 6]> <tibble [0 × 3]>\n```\n\n\n:::\n:::\n\n\nOur best model has a tree_depth of 15 (this minimizes the mean log loss).  \n\nArrange by the mean log loss:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_fits |>\n  collect_metrics() |>\n  arrange(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             <dbl>      <int> <chr>       <chr>      <dbl> <int>   <dbl> <chr>  \n 1    0.0000000001         11 mn_log_loss binary     0.389     5 4.51e-4 Prepro…\n 2    0.0000000178         11 mn_log_loss binary     0.389     5 4.51e-4 Prepro…\n 3    0.00000316           11 mn_log_loss binary     0.389     5 4.51e-4 Prepro…\n 4    0.0000000001         15 mn_log_loss binary     0.389     5 4.51e-4 Prepro…\n 5    0.0000000178         15 mn_log_loss binary     0.389     5 4.51e-4 Prepro…\n 6    0.00000316           15 mn_log_loss binary     0.389     5 4.51e-4 Prepro…\n 7    0.0000000001          8 mn_log_loss binary     0.392     5 2.56e-3 Prepro…\n 8    0.0000000178          8 mn_log_loss binary     0.392     5 2.56e-3 Prepro…\n 9    0.00000316            8 mn_log_loss binary     0.392     5 2.56e-3 Prepro…\n10    0.0000000001          1 mn_log_loss binary     0.402     5 7.11e-4 Prepro…\n# ℹ 15 more rows\n```\n\n\n:::\n:::\n\n\nGrab the best model's tuning parameter values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_best_params <- select_best(tree_fits)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in select_best(tree_fits): No value of `metric` was given;\n\"mn_log_loss\" will be used.\n```\n\n\n:::\n\n```{.r .cell-code}\ntree_best_params\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            <dbl>      <int> <chr>                \n1    0.0000000001         11 Preprocessor1_Model16\n```\n\n\n:::\n:::\n\n\nThese are the values for `tree_depth` and `cost_complexity` that minimize mean log loss in the dbhi data set.\n\nFit this chosen model by `finalize_workflow()` to finalize the model on the training set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_final_wkf <- tree_wkf |>\n  finalize_workflow(tree_best_params)\n\ntree_final_wkf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 11\n  min_n = 10\n\nComputational engine: rpart \n```\n\n\n:::\n:::\n\n\nPerform `last_fit()` on the `dbhi_split` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_final_fit <- tree_final_wkf |>\n  last_fit(dbhi_split, metrics=metric_set(mn_log_loss))\ntree_final_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  <list>                 <chr>         <list>   <list>   <list>       <list>    \n1 <split [177575/76105]> train/test s… <tibble> <tibble> <tibble>     <workflow>\n```\n\n\n:::\n:::\n\n\nThis has information about how the final fitted model - which was fit on the entire training data set - performs on the test set.\n\nLook at the metric with `collect_metrics()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_final_fit |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 mn_log_loss binary         0.396 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nPlot to learn about the fit:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Extract the workflow to better understand the structure of the plot!\ntree_final_model <- extract_workflow(tree_final_fit)\n\n#Plot!\ntree_final_model <- tree_final_model |>\n  extract_fit_engine() |>\n  rpart.plot::rpart.plot(roundint = FALSE,extra=101,digits=-6)\n```\n\n::: {.cell-output-display}\n![](Modeling_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntree_final_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$obj\nn= 177575 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 177575 24553 no diabetes (0.8617317 0.1382683)  \n    2) HeartDiseaseorAttack_yes< 1.386034 160791 19073 no diabetes (0.8813802 0.1186198) *\n    3) HeartDiseaseorAttack_yes>=1.386034 16784  5480 no diabetes (0.6734986 0.3265014)  \n      6) Stroke_yes< 2.322867 13995  4337 no diabetes (0.6901036 0.3098964) *\n      7) Stroke_yes>=2.322867 2789  1143 no diabetes (0.5901757 0.4098243)  \n       14) HvyAlcoholConsump_yes>=1.923615 83    16 no diabetes (0.8072289 0.1927711)  \n         28) Income_X.50.000.to.less.than..75.000>=0.8810366 5     0 no diabetes (1.0000000 0.0000000) *\n         29) Income_X.50.000.to.less.than..75.000< 0.8810366 78    16 no diabetes (0.7948718 0.2051282)  \n           58) Income_X.75.000.or.more>=0.2976892 16     2 no diabetes (0.8750000 0.1250000) *\n           59) Income_X.75.000.or.more< 0.2976892 62    14 no diabetes (0.7741935 0.2258065)  \n            118) Income_X.25.000.to.less.than..35.000>=1.316765 9     1 no diabetes (0.8888889 0.1111111) *\n            119) Income_X.25.000.to.less.than..35.000< 1.316765 53    13 no diabetes (0.7547170 0.2452830)  \n              238) Sex_male>=0.120171 29     6 no diabetes (0.7931034 0.2068966) *\n              239) Sex_male< 0.120171 24     7 no diabetes (0.7083333 0.2916667)  \n                478) Income_X.35.000.to.less.than..50.000< 1.010926 17     3 no diabetes (0.8235294 0.1764706) *\n                479) Income_X.35.000.to.less.than..50.000>=1.010926 7     3 prediabetes or diabetes (0.4285714 0.5714286) *\n       15) HvyAlcoholConsump_yes< 1.923615 2706  1127 no diabetes (0.5835181 0.4164819) *\n\n$snipped.nodes\nNULL\n\n$xlim\n[1] 0 1\n\n$ylim\n[1] 0 1\n\n$x\n [1] 0.23100161 0.04472613 0.41727710 0.15963341 0.67492078 0.38585713\n [7] 0.27454070 0.49717357 0.38944799 0.60489915 0.50435527 0.70544303\n[13] 0.61926256 0.79162349 0.73416985 0.84907714 0.96398442\n\n$y\n [1] 0.95861553 0.02628681 0.85266909 0.02628681 0.74672264 0.64077620\n [7] 0.02628681 0.53482975 0.02628681 0.42888330 0.02628681 0.32293686\n[13] 0.02628681 0.21699041 0.02628681 0.02628681 0.02628681\n\n$branch.x\n       [,1]       [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\nx 0.2310016 0.04472613 0.4172771 0.1596334 0.6749208 0.3858571 0.2745407\n         NA 0.04472613 0.4172771 0.1596334 0.6749208 0.3858571 0.2745407\n         NA 0.23100161 0.2310016 0.4172771 0.4172771 0.6749208 0.3858571\n       [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14]\nx 0.4971736 0.3894480 0.6048992 0.5043553 0.7054430 0.6192626 0.7916235\n  0.4971736 0.3894480 0.6048992 0.5043553 0.7054430 0.6192626 0.7916235\n  0.3858571 0.4971736 0.4971736 0.6048992 0.6048992 0.7054430 0.7054430\n      [,15]     [,16]     [,17]\nx 0.7341698 0.8490771 0.9639844\n  0.7341698 0.8490771 0.9639844\n  0.7916235 0.7916235 0.6749208\n\n$branch.y\n       [,1]       [,2]      [,3]       [,4]      [,5]      [,6]       [,7]\ny 0.9997651 0.06743639 0.8938187 0.06743639 0.7878722 0.6819258 0.06743639\n         NA 0.91327376 0.9132738 0.80732732 0.8073273 0.7013809 0.59543442\n         NA 0.91327376 0.9132738 0.80732732 0.8073273 0.7013809 0.59543442\n       [,8]       [,9]     [,10]      [,11]     [,12]      [,13]     [,14]\ny 0.5759793 0.06743639 0.4700329 0.06743639 0.3640864 0.06743639 0.2581400\n  0.5954344 0.48948798 0.4894880 0.38354153 0.3835415 0.27759509 0.2775951\n  0.5954344 0.48948798 0.4894880 0.38354153 0.3835415 0.27759509 0.2775951\n       [,15]      [,16]      [,17]\ny 0.06743639 0.06743639 0.06743639\n  0.17164864 0.17164864 0.70138087\n  0.17164864 0.17164864 0.70138087\n\n$labs\n [1] \"no diabetes\\n153022  24553\\n100.0000%\" \n [2] \"no diabetes\\n141718  19073\\n90.5482%\"  \n [3] \"no diabetes\\n11304  5480\\n9.4518%\"     \n [4] \"no diabetes\\n9658  4337\\n7.8812%\"      \n [5] \"no diabetes\\n1646  1143\\n1.5706%\"      \n [6] \"no diabetes\\n67  16\\n0.0467%\"          \n [7] \"no diabetes\\n5  0\\n0.0028%\"            \n [8] \"no diabetes\\n62  16\\n0.0439%\"          \n [9] \"no diabetes\\n14  2\\n0.0090%\"           \n[10] \"no diabetes\\n48  14\\n0.0349%\"          \n[11] \"no diabetes\\n8  1\\n0.0051%\"            \n[12] \"no diabetes\\n40  13\\n0.0298%\"          \n[13] \"no diabetes\\n23  6\\n0.0163%\"           \n[14] \"no diabetes\\n17  7\\n0.0135%\"           \n[15] \"no diabetes\\n14  3\\n0.0096%\"           \n[16] \"prediabetes or diabetes\\n3  4\\n0.0039%\"\n[17] \"no diabetes\\n1579  1127\\n1.5239%\"      \n\n$cex\n[1] 0.4625\n\n$boxes\n$boxes$x1\n [1]  0.183660439 -0.002615046  0.376951032  0.121767869  0.637055236\n [6]  0.347991591  0.236675157  0.459308026  0.351582444  0.567033608\n[11]  0.466489731  0.667577484  0.581397019  0.753757950  0.696304306\n[16]  0.777906774  0.926118881\n\n$boxes$y1\n [1]  0.930420747 -0.001907977  0.824474301 -0.001907977  0.718527855\n [6]  0.612581409 -0.001907977  0.506634963 -0.001907977  0.400688517\n[11] -0.001907977  0.294742071 -0.001907977  0.188795625 -0.001907977\n[16] -0.001907977 -0.001907977\n\n$boxes$x2\n [1] 0.2783428 0.0920673 0.4576032 0.1974990 0.7127863 0.4237227 0.3124062\n [8] 0.5350391 0.4273135 0.6427647 0.5422208 0.7433086 0.6571281 0.8294890\n[15] 0.7720354 0.9202475 1.0018500\n\n$boxes$y2\n [1] 0.99976511 0.06743639 0.89381867 0.06743639 0.78787222 0.68192578\n [7] 0.06743639 0.57597933 0.06743639 0.47003288 0.06743639 0.36408644\n[13] 0.06743639 0.25813999 0.06743639 0.06743639 0.06743639\n\n\n$split.labs\n[1] \"\"\n\n$split.cex\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n$split.box\n$split.box$x1\n [1] 0.1161793        NA 0.3483794        NA 0.5581954 0.2329999        NA\n [8] 0.3786743        NA 0.4555495        NA 0.6339093        NA 0.6459569\n[15]        NA        NA        NA\n\n$split.box$y1\n [1] 0.9003190        NA 0.7943725        NA 0.6884261 0.5824796        NA\n [8] 0.4765332        NA 0.3705867        NA 0.2646403        NA 0.1586938\n[15]        NA        NA        NA\n\n$split.box$x2\n [1] 0.3458239        NA 0.4861748        NA 0.7916462 0.5387144        NA\n [8] 0.6156728        NA 0.7542488        NA 0.7769768        NA 0.9372901\n[15]        NA        NA        NA\n\n$split.box$y2\n [1] 0.9262286        NA 0.8202821        NA 0.7143357 0.6083892        NA\n [8] 0.5024428        NA 0.3964963        NA 0.2905499        NA 0.1846034\n[15]        NA        NA        NA\n```\n\n\n:::\n:::\n\n\n\n### Random Forest\n\nYou should provide a thorough explanation of what a random forest is and why we might use it (be sure to relate this to a basic classification tree). You should then fit a random forest model with varying values for the `mtry` parameter and choose the best model (based on 5 fold CV on the training set). Include at least 5 predictors in this model.\n\n\n::: {.cell}\n\n:::\n\n\n## Final Model Selection\n\nWe will compare the best models from each of the approaches on the test set and declare an overall winner...\n\nGet the random forest (rf) specification:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(mtry = tune()) |>\n set_engine(\"ranger\", importance=\"impurity\") |>\n set_mode(\"classification\")\n```\n:::\n\n\nCreate the workflow using the same recipe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_wkf <- workflow() |>\n add_recipe(dbhi_recipe) |>\n add_model(rf_spec)\n```\n:::\n\n\nFit to the cross-validation folds:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_fit <- rf_wkf |>\n tune_grid(resamples = dbhi_5_fold,\n           grid = 7,\n           metrics = metric_set(mn_log_loss))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n\n\n:::\n:::\n\n\nArrange by the mean log loss: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_fit |>\n collect_metrics() |>\n arrange(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  <int> <chr>       <chr>      <dbl> <int>    <dbl> <chr>               \n1     9 mn_log_loss binary     0.375     5 0.000680 Preprocessor1_Model6\n2     8 mn_log_loss binary     0.375     5 0.000696 Preprocessor1_Model7\n3    10 mn_log_loss binary     0.375     5 0.000663 Preprocessor1_Model3\n4     6 mn_log_loss binary     0.376     5 0.000737 Preprocessor1_Model2\n5     4 mn_log_loss binary     0.376     5 0.000772 Preprocessor1_Model5\n6     3 mn_log_loss binary     0.376     5 0.000757 Preprocessor1_Model1\n7     2 mn_log_loss binary     0.378     5 0.000729 Preprocessor1_Model4\n```\n\n\n:::\n:::\n\n\nObtain the best tuning parameter, use it to refit on the entire training set, then extract the final model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_best_params <- select_best(rf_fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in select_best(rf_fit): No value of `metric` was given; \"mn_log_loss\"\nwill be used.\n```\n\n\n:::\n\n```{.r .cell-code}\nrf_best_params\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n   mtry .config             \n  <int> <chr>               \n1     9 Preprocessor1_Model6\n```\n\n\n:::\n\n```{.r .cell-code}\nrf_final_wkf <- rf_wkf |>\n  finalize_workflow(rf_best_params)\n\nrf_final_fit <- rf_final_wkf |>\n  last_fit(dbhi_split, metrics = metric_set(mn_log_loss))\n\n#Investigate the random forest model\n#Refit to the entire data set\nrf_full_fit <- rf_final_wkf |>\n  fit(dbhi_data)\nrf_full_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~9L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      253680 \nNumber of independent variables:  11 \nMtry:                             9 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1128596 \n```\n\n\n:::\n\n```{.r .cell-code}\n#rf_final_model <- extract_fit_engine(rf_full_fit)\nrf_final_model <- extract_fit_engine(rf_final_fit)\nattributes(rf_final_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$names\n [1] \"predictions\"               \"num.trees\"                \n [3] \"num.independent.variables\" \"mtry\"                     \n [5] \"min.node.size\"             \"variable.importance\"      \n [7] \"prediction.error\"          \"forest\"                   \n [9] \"splitrule\"                 \"treetype\"                 \n[11] \"call\"                      \"importance.mode\"          \n[13] \"num.samples\"               \"replace\"                  \n[15] \"max.depth\"                \n\n$class\n[1] \"ranger\"\n```\n\n\n:::\n:::\n\n\nCollect the metrics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_final_fit |> collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 mn_log_loss binary         0.381 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nProduce a variable importance plot to examine the final model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimp <- enframe(rf_final_model$variable.importance,\n        name = \"variable\",\n        value = \"importance\")\nggplot(imp, aes(x = reorder(variable, -importance), y = importance)) +\n  geom_bar(stat = 'identity') + \n  xlab('term') +\n  ylab('value') +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](Modeling_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "Modeling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}